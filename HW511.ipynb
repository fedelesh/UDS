{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from validation import cross_val, Preprocessing, cross_gen\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_cleaned.csv',sep = '\\t')\n",
    "test = pd.read_csv('test_cleaned.csv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('0',axis=1)\n",
    "y= train['0']\n",
    "Xt = test.drop('0',axis=1)\n",
    "yt = test['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.12, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "cv_score = np.mean(cross_val_score(lr,X_train, y_train, scoring='roc_auc',n_jobs=-1))\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = lr.predict_proba(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7312271026155143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred[:,1] )#0.73486478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_2 = pd.DataFrame({'_ID_': yt.index})\n",
    "submission_2['_VAL_'] = y_pred2[:,1]\n",
    "submission_2.to_csv('submission_log1234.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  9.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': [4, 7, 10, 13], 'min_samples_leaf': [1, 3, 5, 7], 'max_depth': [10, 15, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'max_features': [4,7, 10, 13], 'min_samples_leaf': [1,3,5,7], 'max_depth': [10,15,20]}\n",
    "rfc = RandomForestClassifier(n_estimators =100 ,random_state=42, n_jobs=-1)\n",
    "gcv = GridSearchCV(rfc, parameters, n_jobs=-1, cv=skf, verbose=1,  scoring ='roc_auc')\n",
    "gcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_h = best_rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738161457355585"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_h[:,1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc = best_rf.predict_proba(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) 337                            0.028377 \n",
      " 2) 294                            0.021112 \n",
      " 3) 86                             0.018942 \n",
      " 4) 336                            0.018328 \n",
      " 5) 4                              0.017776 \n",
      " 6) 144                            0.016509 \n",
      " 7) 34                             0.016101 \n",
      " 8) 60                             0.015402 \n",
      " 9) 65                             0.014247 \n",
      "10) 214                            0.012226 \n",
      "11) 91                             0.011393 \n",
      "12) 332                            0.011266 \n",
      "13) 324                            0.011051 \n",
      "14) 96                             0.010529 \n",
      "15) 334                            0.010457 \n",
      "16) 36                             0.010260 \n",
      "17) 88                             0.010243 \n",
      "18) 325                            0.009739 \n",
      "19) 323                            0.009696 \n",
      "20) 331                            0.008997 \n",
      "21) 282                            0.008960 \n",
      "22) 344                            0.008522 \n",
      "23) 118                            0.008514 \n",
      "24) 147                            0.008025 \n",
      "25) 318                            0.008010 \n",
      "26) 322                            0.007804 \n",
      "27) 72                             0.007747 \n",
      "28) 62                             0.007696 \n",
      "29) 314                            0.007676 \n",
      "30) 97                             0.007615 \n",
      "31) 121                            0.007547 \n",
      "32) 194                            0.007263 \n",
      "33) 114                            0.007077 \n",
      "34) 46                             0.007030 \n",
      "35) 327                            0.006941 \n",
      "36) 321                            0.006911 \n",
      "37) 219                            0.006866 \n",
      "38) 13                             0.006730 \n",
      "39) 326                            0.006688 \n",
      "40) 201                            0.006627 \n",
      "41) 116                            0.006281 \n",
      "42) 122                            0.006149 \n",
      "43) 204                            0.005996 \n",
      "44) 125                            0.005957 \n",
      "45) 117                            0.005881 \n",
      "46) 212                            0.005865 \n",
      "47) 158                            0.005854 \n",
      "48) 112                            0.005778 \n",
      "49) 111                            0.005699 \n",
      "50) 329                            0.005679 \n",
      "51) 119                            0.005675 \n",
      "52) 296                            0.005604 \n",
      "53) 115                            0.005548 \n",
      "54) 126                            0.005507 \n",
      "55) 120                            0.005469 \n",
      "56) 216                            0.005423 \n",
      "57) 99                             0.005420 \n",
      "58) 98                             0.005414 \n",
      "59) 48                             0.005411 \n",
      "60) 127                            0.005399 \n",
      "61) 113                            0.005352 \n",
      "62) 61                             0.005352 \n",
      "63) 100                            0.005335 \n",
      "64) 328                            0.005318 \n",
      "65) 205                            0.005282 \n",
      "66) 109                            0.005268 \n",
      "67) 195                            0.005250 \n",
      "68) 110                            0.005233 \n",
      "69) 209                            0.005201 \n",
      "70) 165                            0.005190 \n",
      "71) 196                            0.005097 \n",
      "72) 208                            0.005069 \n",
      "73) 213                            0.005056 \n",
      "74) 101                            0.005031 \n",
      "75) 313                            0.005029 \n",
      "76) 210                            0.005021 \n",
      "77) 94                             0.004949 \n",
      "78) 20                             0.004900 \n",
      "79) 207                            0.004828 \n",
      "80) 319                            0.004791 \n",
      "81) 333                            0.004783 \n",
      "82) 10                             0.004764 \n",
      "83) 218                            0.004706 \n",
      "84) 202                            0.004702 \n",
      "85) 310                            0.004657 \n",
      "86) 87                             0.004639 \n",
      "87) 335                            0.004624 \n",
      "88) 95                             0.004597 \n",
      "89) 330                            0.004546 \n",
      "90) 103                            0.004520 \n",
      "91) 206                            0.004498 \n",
      "92) 123                            0.004437 \n",
      "93) 197                            0.004396 \n",
      "94) 199                            0.004350 \n",
      "95) 312                            0.004349 \n",
      "96) 203                            0.004296 \n",
      "97) 198                            0.004254 \n",
      "98) 222                            0.004225 \n",
      "99) 22                             0.004218 \n",
      "100) 92                             0.004198 \n",
      "101) 105                            0.004182 \n",
      "102) 217                            0.004173 \n",
      "103) 104                            0.004171 \n",
      "104) 124                            0.004167 \n",
      "105) 200                            0.004155 \n",
      "106) 107                            0.004121 \n",
      "107) 35                             0.004117 \n",
      "108) 51                             0.003993 \n",
      "109) 64                             0.003972 \n",
      "110) 146                            0.003942 \n",
      "111) 63                             0.003932 \n",
      "112) 297                            0.003927 \n",
      "113) 148                            0.003898 \n",
      "114) 338                            0.003896 \n",
      "115) 47                             0.003892 \n",
      "116) 340                            0.003865 \n",
      "117) 102                            0.003815 \n",
      "118) 320                            0.003807 \n",
      "119) 108                            0.003795 \n",
      "120) 39                             0.003732 \n",
      "121) 311                            0.003699 \n",
      "122) 44                             0.003683 \n",
      "123) 106                            0.003665 \n",
      "124) 93                             0.003569 \n",
      "125) 57                             0.003547 \n",
      "126) 343                            0.003546 \n",
      "127) 316                            0.003497 \n",
      "128) 211                            0.003395 \n",
      "129) 49                             0.003368 \n",
      "130) 25                             0.003278 \n",
      "131) 41                             0.003185 \n",
      "132) 45                             0.003179 \n",
      "133) 232                            0.003161 \n",
      "134) 74                             0.003115 \n",
      "135) 7                              0.003052 \n",
      "136) 317                            0.003039 \n",
      "137) 73                             0.002951 \n",
      "138) 89                             0.002855 \n",
      "139) 70                             0.002792 \n",
      "140) 341                            0.002727 \n",
      "141) 145                            0.002702 \n",
      "142) 190                            0.002644 \n",
      "143) 50                             0.002638 \n",
      "144) 77                             0.002597 \n",
      "145) 221                            0.002544 \n",
      "146) 59                             0.002458 \n",
      "147) 83                             0.002413 \n",
      "148) 37                             0.002370 \n",
      "149) 71                             0.002319 \n",
      "150) 42                             0.002243 \n",
      "151) 67                             0.002114 \n",
      "152) 56                             0.002106 \n",
      "153) 90                             0.002103 \n",
      "154) 1                              0.002101 \n",
      "155) 2                              0.001948 \n",
      "156) 171                            0.001883 \n",
      "157) 142                            0.001841 \n",
      "158) 267                            0.001836 \n",
      "159) 273                            0.001815 \n",
      "160) 38                             0.001794 \n",
      "161) 75                             0.001659 \n",
      "162) 21                             0.001637 \n",
      "163) 18                             0.001590 \n",
      "164) 31                             0.001590 \n",
      "165) 251                            0.001523 \n",
      "166) 85                             0.001498 \n",
      "167) 230                            0.001452 \n",
      "168) 76                             0.001405 \n",
      "169) 238                            0.001275 \n",
      "170) 82                             0.001237 \n",
      "171) 24                             0.001237 \n",
      "172) 23                             0.001213 \n",
      "173) 176                            0.001211 \n",
      "174) 55                             0.001192 \n",
      "175) 177                            0.001169 \n",
      "176) 342                            0.001165 \n",
      "177) 40                             0.001157 \n",
      "178) 189                            0.001010 \n",
      "179) 166                            0.001002 \n",
      "180) 277                            0.001000 \n",
      "181) 298                            0.000964 \n",
      "182) 285                            0.000941 \n",
      "183) 259                            0.000938 \n",
      "184) 179                            0.000891 \n",
      "185) 272                            0.000878 \n",
      "186) 15                             0.000869 \n",
      "187) 52                             0.000860 \n",
      "188) 68                             0.000853 \n",
      "189) 151                            0.000852 \n",
      "190) 19                             0.000848 \n",
      "191) 54                             0.000844 \n",
      "192) 167                            0.000817 \n",
      "193) 58                             0.000793 \n",
      "194) 253                            0.000776 \n",
      "195) 43                             0.000766 \n",
      "196) 264                            0.000759 \n",
      "197) 33                             0.000753 \n",
      "198) 239                            0.000747 \n",
      "199) 291                            0.000747 \n",
      "200) 241                            0.000744 \n",
      "201) 280                            0.000737 \n",
      "202) 270                            0.000702 \n",
      "203) 247                            0.000677 \n",
      "204) 173                            0.000660 \n",
      "205) 252                            0.000658 \n",
      "206) 12                             0.000653 \n",
      "207) 30                             0.000648 \n",
      "208) 168                            0.000629 \n",
      "209) 281                            0.000622 \n",
      "210) 159                            0.000609 \n",
      "211) 220                            0.000607 \n",
      "212) 224                            0.000598 \n",
      "213) 254                            0.000595 \n",
      "214) 215                            0.000593 \n",
      "215) 237                            0.000583 \n",
      "216) 250                            0.000575 \n",
      "217) 283                            0.000569 \n",
      "218) 14                             0.000549 \n",
      "219) 187                            0.000540 \n",
      "220) 293                            0.000527 \n",
      "221) 66                             0.000526 \n",
      "222) 81                             0.000505 \n",
      "223) 78                             0.000479 \n",
      "224) 53                             0.000468 \n",
      "225) 261                            0.000429 \n",
      "226) 249                            0.000425 \n",
      "227) 84                             0.000414 \n",
      "228) 274                            0.000405 \n",
      "229) 276                            0.000397 \n",
      "230) 289                            0.000387 \n",
      "231) 265                            0.000378 \n",
      "232) 80                             0.000374 \n",
      "233) 155                            0.000370 \n",
      "234) 234                            0.000367 \n",
      "235) 292                            0.000352 \n",
      "236) 240                            0.000351 \n",
      "237) 8                              0.000340 \n",
      "238) 236                            0.000339 \n",
      "239) 136                            0.000337 \n",
      "240) 32                             0.000321 \n",
      "241) 69                             0.000316 \n",
      "242) 255                            0.000305 \n",
      "243) 128                            0.000295 \n",
      "244) 248                            0.000294 \n",
      "245) 288                            0.000287 \n",
      "246) 242                            0.000273 \n",
      "247) 16                             0.000271 \n",
      "248) 226                            0.000266 \n",
      "249) 302                            0.000260 \n",
      "250) 245                            0.000248 \n",
      "251) 262                            0.000245 \n",
      "252) 79                             0.000244 \n",
      "253) 184                            0.000225 \n",
      "254) 28                             0.000216 \n",
      "255) 275                            0.000211 \n",
      "256) 163                            0.000196 \n",
      "257) 26                             0.000195 \n",
      "258) 29                             0.000191 \n",
      "259) 315                            0.000176 \n",
      "260) 134                            0.000173 \n",
      "261) 132                            0.000172 \n",
      "262) 256                            0.000168 \n",
      "263) 180                            0.000166 \n",
      "264) 6                              0.000165 \n",
      "265) 143                            0.000149 \n",
      "266) 244                            0.000147 \n",
      "267) 225                            0.000143 \n",
      "268) 183                            0.000134 \n",
      "269) 27                             0.000127 \n",
      "270) 260                            0.000121 \n",
      "271) 169                            0.000109 \n",
      "272) 228                            0.000104 \n",
      "273) 263                            0.000101 \n",
      "274) 287                            0.000099 \n",
      "275) 269                            0.000098 \n",
      "276) 278                            0.000091 \n",
      "277) 227                            0.000085 \n",
      "278) 243                            0.000083 \n",
      "279) 154                            0.000082 \n",
      "280) 223                            0.000074 \n",
      "281) 304                            0.000073 \n",
      "282) 17                             0.000060 \n",
      "283) 175                            0.000060 \n",
      "284) 309                            0.000054 \n",
      "285) 286                            0.000052 \n",
      "286) 271                            0.000052 \n",
      "287) 308                            0.000049 \n",
      "288) 295                            0.000044 \n",
      "289) 235                            0.000036 \n",
      "290) 306                            0.000035 \n",
      "291) 181                            0.000035 \n",
      "292) 266                            0.000034 \n",
      "293) 339                            0.000032 \n",
      "294) 156                            0.000031 \n",
      "295) 233                            0.000031 \n",
      "296) 300                            0.000030 \n",
      "297) 257                            0.000027 \n",
      "298) 133                            0.000027 \n",
      "299) 172                            0.000026 \n",
      "300) 131                            0.000026 \n",
      "301) 258                            0.000024 \n",
      "302) 174                            0.000023 \n",
      "303) 279                            0.000022 \n",
      "304) 229                            0.000021 \n",
      "305) 161                            0.000020 \n",
      "306) 139                            0.000014 \n",
      "307) 157                            0.000011 \n",
      "308) 290                            0.000010 \n",
      "309) 307                            0.000010 \n",
      "310) 284                            0.000009 \n",
      "311) 162                            0.000005 \n",
      "312) 135                            0.000004 \n",
      "313) 268                            0.000004 \n",
      "314) 231                            0.000002 \n",
      "315) 246                            0.000001 \n",
      "316) 170                            0.000000 \n",
      "317) 185                            0.000000 \n",
      "318) 182                            0.000000 \n",
      "319) 299                            0.000000 \n",
      "320) 191                            0.000000 \n",
      "321) 3                              0.000000 \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 321 is out of bounds for axis 0 with size 321",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-fdb451d13302>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.000011\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mcol_rfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"%2d) %- *s %f \"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mfeat_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 321 is out of bounds for axis 0 with size 321"
     ]
    }
   ],
   "source": [
    "col_rfc = []\n",
    "importances = best_etc.feature_importances_\n",
    "feat_labels = X_train.columns[0:]\n",
    "indices = np . argsort(importances)[::-1 ] \n",
    "for f in range(X_train .shape[1]):\n",
    "    if importances[indices[f]] < 0.000011:\n",
    "        col_rfc.append(feat_labels[indices[f]])\n",
    "    print (\"%2d) %- *s %f \" % (f + 1, 30 , feat_labels[indices[f]] , importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=15, max_features=13, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=7, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_best_rfc = X_train.drop(col_rfc,axis=1)\n",
    "X_test_best_rfc = X_test.drop(col_rfc,axis=1)\n",
    "best_rf.fit(X_best_rfc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hrfc2 = best_rf.predict_proba(X_test_best_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738287134976187"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_hrfc2[:,1])#0.73568169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_best_rfc = Xt.drop(col_rfc,axis=1)\n",
    "y_pred_rfc2 = best_rf.predict_proba(Xt_best_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_rfc = pd.DataFrame({'_ID_': yt.index})\n",
    "submission_rfc['_VAL_'] = y_pred_rfc2[:,1]\n",
    "submission_rfc.to_csv('submission_rf12345.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "          error_score='raise',\n",
       "          estimator=BaggingClassifier(base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start..., n_estimators=25, n_jobs=1, oob_score=False,\n",
       "         random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'max_features': [0.5, 0.7, 0.9, 1.0], 'max_samples': [0.5, 0.7, 0.9, 1.0], 'base_estimator__C': [0.005, 0.001, 0.01]},\n",
       "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'max_features': [0.5, 0.7, 0.9, 1.], 'max_samples': [0.5, 0.7, 0.9, 1.], \"base_estimator__C\": [0.005, 0.001, 0.01]}\n",
    "bg = BaggingClassifier(lr, random_state=42, n_estimators=25)\n",
    "r_grid_search = RandomizedSearchCV(bg, parameters, scoring ='roc_auc', n_iter=10, cv=skf, random_state=1)\n",
    "r_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bg = r_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hbg = best_bg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7271123436983189"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_hbg[:,1])#0.72539552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bg = best_bg.predict_proba(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_bg = pd.DataFrame({'_ID_': yt.index})\n",
    "submission_bg['_VAL_'] = y_pred_bg[:,1]\n",
    "submission_bg.to_csv('submission_bg1234.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   45.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': [3, 4, 5, 6, 7], 'min_samples_leaf': [1, 3, 5], 'max_depth': [5, 10, 15, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'max_features': [3, 4, 5, 6, 7], 'min_samples_leaf': [1, 3, 5], 'max_depth': [5,10,15,20]}\n",
    "etc = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "etcv = GridSearchCV(etc, parameters, n_jobs=-1, cv=skf, verbose=1, scoring='roc_auc')\n",
    "etcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_etc = etcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hetc = best_etc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.747267563447479"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_hetc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_etc = best_etc.predict_proba(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) 295                            0.021380 \n",
      " 2) 337                            0.020391 \n",
      " 3) 86                             0.019949 \n",
      " 4) 338                            0.017908 \n",
      " 5) 144                            0.017284 \n",
      " 6) 34                             0.015963 \n",
      " 7) 335                            0.015866 \n",
      " 8) 65                             0.015720 \n",
      " 9) 60                             0.015114 \n",
      "10) 4                              0.014893 \n",
      "11) 96                             0.013207 \n",
      "12) 333                            0.011328 \n",
      "13) 147                            0.010976 \n",
      "14) 88                             0.010823 \n",
      "15) 324                            0.010497 \n",
      "16) 91                             0.010475 \n",
      "17) 325                            0.010089 \n",
      "18) 283                            0.009831 \n",
      "19) 214                            0.009828 \n",
      "20) 323                            0.009417 \n",
      "21) 118                            0.008745 \n",
      "22) 62                             0.008613 \n",
      "23) 319                            0.008570 \n",
      "24) 97                             0.008451 \n",
      "25) 345                            0.008434 \n",
      "26) 46                             0.008369 \n",
      "27) 315                            0.008362 \n",
      "28) 194                            0.008265 \n",
      "29) 329                            0.008082 \n",
      "30) 332                            0.008002 \n",
      "31) 36                             0.007716 \n",
      "32) 322                            0.007614 \n",
      "33) 328                            0.007452 \n",
      "34) 219                            0.007270 \n",
      "35) 111                            0.007261 \n",
      "36) 327                            0.007258 \n",
      "37) 121                            0.007022 \n",
      "38) 116                            0.006866 \n",
      "39) 114                            0.006782 \n",
      "40) 201                            0.006709 \n",
      "41) 13                             0.006647 \n",
      "42) 298                            0.006627 \n",
      "43) 115                            0.006519 \n",
      "44) 119                            0.006475 \n",
      "45) 126                            0.006472 \n",
      "46) 72                             0.006469 \n",
      "47) 22                             0.006145 \n",
      "48) 125                            0.006144 \n",
      "49) 127                            0.005938 \n",
      "50) 326                            0.005921 \n",
      "51) 117                            0.005613 \n",
      "52) 314                            0.005564 \n",
      "53) 216                            0.005538 \n",
      "54) 20                             0.005528 \n",
      "55) 61                             0.005522 \n",
      "56) 210                            0.005417 \n",
      "57) 100                            0.005393 \n",
      "58) 212                            0.005336 \n",
      "59) 101                            0.005334 \n",
      "60) 98                             0.005328 \n",
      "61) 122                            0.005306 \n",
      "62) 158                            0.005243 \n",
      "63) 99                             0.005151 \n",
      "64) 109                            0.005021 \n",
      "65) 341                            0.005010 \n",
      "66) 207                            0.005002 \n",
      "67) 199                            0.004973 \n",
      "68) 112                            0.004956 \n",
      "69) 120                            0.004942 \n",
      "70) 297                            0.004938 \n",
      "71) 204                            0.004914 \n",
      "72) 330                            0.004913 \n",
      "73) 209                            0.004891 \n",
      "74) 48                             0.004832 \n",
      "75) 208                            0.004816 \n",
      "76) 123                            0.004789 \n",
      "77) 218                            0.004775 \n",
      "78) 205                            0.004758 \n",
      "79) 336                            0.004728 \n",
      "80) 195                            0.004727 \n",
      "81) 87                             0.004722 \n",
      "82) 344                            0.004714 \n",
      "83) 213                            0.004662 \n",
      "84) 222                            0.004655 \n",
      "85) 334                            0.004637 \n",
      "86) 94                             0.004628 \n",
      "87) 196                            0.004613 \n",
      "88) 110                            0.004521 \n",
      "89) 10                             0.004514 \n",
      "90) 203                            0.004463 \n",
      "91) 202                            0.004444 \n",
      "92) 198                            0.004405 \n",
      "93) 200                            0.004402 \n",
      "94) 217                            0.004373 \n",
      "95) 206                            0.004330 \n",
      "96) 103                            0.004322 \n",
      "97) 113                            0.004298 \n",
      "98) 105                            0.004283 \n",
      "99) 95                             0.004273 \n",
      "100) 107                            0.004224 \n",
      "101) 104                            0.004188 \n",
      "102) 47                             0.004172 \n",
      "103) 92                             0.004170 \n",
      "104) 148                            0.004138 \n",
      "105) 313                            0.004098 \n",
      "106) 312                            0.004077 \n",
      "107) 223                            0.004047 \n",
      "108) 108                            0.004009 \n",
      "109) 146                            0.004000 \n",
      "110) 51                             0.003967 \n",
      "111) 63                             0.003929 \n",
      "112) 102                            0.003870 \n",
      "113) 124                            0.003869 \n",
      "114) 106                            0.003817 \n",
      "115) 331                            0.003802 \n",
      "116) 197                            0.003793 \n",
      "117) 64                             0.003753 \n",
      "118) 165                            0.003605 \n",
      "119) 45                             0.003589 \n",
      "120) 39                             0.003534 \n",
      "121) 93                             0.003521 \n",
      "122) 35                             0.003504 \n",
      "123) 317                            0.003473 \n",
      "124) 57                             0.003431 \n",
      "125) 311                            0.003370 \n",
      "126) 320                            0.003364 \n",
      "127) 41                             0.003319 \n",
      "128) 7                              0.003251 \n",
      "129) 44                             0.003242 \n",
      "130) 321                            0.003142 \n",
      "131) 339                            0.003137 \n",
      "132) 211                            0.003099 \n",
      "133) 25                             0.003090 \n",
      "134) 49                             0.002988 \n",
      "135) 74                             0.002924 \n",
      "136) 268                            0.002893 \n",
      "137) 145                            0.002877 \n",
      "138) 221                            0.002789 \n",
      "139) 89                             0.002666 \n",
      "140) 73                             0.002661 \n",
      "141) 70                             0.002623 \n",
      "142) 318                            0.002562 \n",
      "143) 233                            0.002560 \n",
      "144) 59                             0.002537 \n",
      "145) 190                            0.002524 \n",
      "146) 342                            0.002515 \n",
      "147) 83                             0.002487 \n",
      "148) 50                             0.002411 \n",
      "149) 90                             0.002339 \n",
      "150) 37                             0.002247 \n",
      "151) 67                             0.002203 \n",
      "152) 77                             0.002124 \n",
      "153) 1                              0.002080 \n",
      "154) 2                              0.002063 \n",
      "155) 142                            0.002040 \n",
      "156) 71                             0.001998 \n",
      "157) 18                             0.001980 \n",
      "158) 42                             0.001952 \n",
      "159) 56                             0.001913 \n",
      "160) 171                            0.001905 \n",
      "161) 21                             0.001810 \n",
      "162) 274                            0.001779 \n",
      "163) 31                             0.001721 \n",
      "164) 75                             0.001541 \n",
      "165) 38                             0.001375 \n",
      "166) 76                             0.001308 \n",
      "167) 252                            0.001284 \n",
      "168) 85                             0.001240 \n",
      "169) 23                             0.001181 \n",
      "170) 82                             0.001172 \n",
      "171) 40                             0.001164 \n",
      "172) 231                            0.001159 \n",
      "173) 24                             0.001158 \n",
      "174) 177                            0.001152 \n",
      "175) 55                             0.001131 \n",
      "176) 189                            0.001101 \n",
      "177) 68                             0.001099 \n",
      "178) 299                            0.001088 \n",
      "179) 239                            0.001062 \n",
      "180) 176                            0.001012 \n",
      "181) 273                            0.001000 \n",
      "182) 278                            0.000995 \n",
      "183) 167                            0.000987 \n",
      "184) 15                             0.000986 \n",
      "185) 19                             0.000984 \n",
      "186) 52                             0.000980 \n",
      "187) 260                            0.000927 \n",
      "188) 343                            0.000923 \n",
      "189) 43                             0.000919 \n",
      "190) 166                            0.000904 \n",
      "191) 286                            0.000894 \n",
      "192) 54                             0.000867 \n",
      "193) 151                            0.000796 \n",
      "194) 265                            0.000789 \n",
      "195) 58                             0.000752 \n",
      "196) 271                            0.000729 \n",
      "197) 292                            0.000728 \n",
      "198) 168                            0.000715 \n",
      "199) 253                            0.000714 \n",
      "200) 238                            0.000699 \n",
      "201) 254                            0.000698 \n",
      "202) 30                             0.000650 \n",
      "203) 173                            0.000645 \n",
      "204) 281                            0.000628 \n",
      "205) 179                            0.000628 \n",
      "206) 251                            0.000595 \n",
      "207) 33                             0.000568 \n",
      "208) 240                            0.000568 \n",
      "209) 220                            0.000560 \n",
      "210) 215                            0.000559 \n",
      "211) 282                            0.000552 \n",
      "212) 294                            0.000540 \n",
      "213) 248                            0.000529 \n",
      "214) 66                             0.000521 \n",
      "215) 250                            0.000515 \n",
      "216) 284                            0.000512 \n",
      "217) 277                            0.000506 \n",
      "218) 53                             0.000504 \n",
      "219) 266                            0.000503 \n",
      "220) 187                            0.000501 \n",
      "221) 136                            0.000493 \n",
      "222) 262                            0.000493 \n",
      "223) 81                             0.000490 \n",
      "224) 80                             0.000490 \n",
      "225) 78                             0.000483 \n",
      "226) 275                            0.000478 \n",
      "227) 225                            0.000472 \n",
      "228) 84                             0.000465 \n",
      "229) 159                            0.000452 \n",
      "230) 12                             0.000439 \n",
      "231) 255                            0.000434 \n",
      "232) 69                             0.000425 \n",
      "233) 256                            0.000422 \n",
      "234) 249                            0.000399 \n",
      "235) 289                            0.000387 \n",
      "236) 241                            0.000384 \n",
      "237) 14                             0.000376 \n",
      "238) 242                            0.000376 \n",
      "239) 128                            0.000372 \n",
      "240) 155                            0.000371 \n",
      "241) 293                            0.000364 \n",
      "242) 263                            0.000323 \n",
      "243) 184                            0.000316 \n",
      "244) 290                            0.000294 \n",
      "245) 163                            0.000293 \n",
      "246) 237                            0.000291 \n",
      "247) 235                            0.000283 \n",
      "248) 246                            0.000263 \n",
      "249) 16                             0.000252 \n",
      "250) 227                            0.000237 \n",
      "251) 29                             0.000227 \n",
      "252) 28                             0.000222 \n",
      "253) 132                            0.000203 \n",
      "254) 304                            0.000200 \n",
      "255) 245                            0.000200 \n",
      "256) 32                             0.000184 \n",
      "257) 79                             0.000183 \n",
      "258) 243                            0.000175 \n",
      "259) 8                              0.000171 \n",
      "260) 276                            0.000168 \n",
      "261) 26                             0.000167 \n",
      "262) 316                            0.000162 \n",
      "263) 134                            0.000162 \n",
      "264) 226                            0.000158 \n",
      "265) 257                            0.000156 \n",
      "266) 154                            0.000149 \n",
      "267) 180                            0.000140 \n",
      "268) 6                              0.000127 \n",
      "269) 143                            0.000112 \n",
      "270) 27                             0.000107 \n",
      "271) 229                            0.000101 \n",
      "272) 228                            0.000099 \n",
      "273) 279                            0.000099 \n",
      "274) 261                            0.000094 \n",
      "275) 169                            0.000082 \n",
      "276) 175                            0.000075 \n",
      "277) 270                            0.000072 \n",
      "278) 183                            0.000072 \n",
      "279) 272                            0.000067 \n",
      "280) 244                            0.000058 \n",
      "281) 287                            0.000055 \n",
      "282) 174                            0.000055 \n",
      "283) 264                            0.000053 \n",
      "284) 156                            0.000052 \n",
      "285) 309                            0.000052 \n",
      "286) 17                             0.000048 \n",
      "287) 172                            0.000048 \n",
      "288) 340                            0.000047 \n",
      "289) 181                            0.000047 \n",
      "290) 306                            0.000040 \n",
      "291) 131                            0.000036 \n",
      "292) 139                            0.000036 \n",
      "293) 224                            0.000035 \n",
      "294) 236                            0.000035 \n",
      "295) 280                            0.000033 \n",
      "296) 288                            0.000033 \n",
      "297) 234                            0.000029 \n",
      "298) 157                            0.000027 \n",
      "299) 310                            0.000024 \n",
      "300) 258                            0.000023 \n",
      "301) 296                            0.000022 \n",
      "302) 307                            0.000021 \n",
      "303) 133                            0.000020 \n",
      "304) 291                            0.000019 \n",
      "305) 161                            0.000018 \n",
      "306) 302                            0.000015 \n",
      "307) 259                            0.000012 \n",
      "308) 285                            0.000011 \n",
      "309) 267                            0.000007 \n",
      "310) 162                            0.000005 \n",
      "311) 247                            0.000003 \n",
      "312) 308                            0.000002 \n",
      "313) 135                            0.000002 \n",
      "314) 230                            0.000001 \n",
      "315) 300                            0.000000 \n",
      "316) 269                            0.000000 \n",
      "317) 232                            0.000000 \n",
      "318) 191                            0.000000 \n",
      "319) 170                            0.000000 \n",
      "320) 3                              0.000000 \n",
      "321) 182                            0.000000 \n",
      "322) 185                            0.000000 \n"
     ]
    }
   ],
   "source": [
    "col_etc = []\n",
    "importances = best_etc.feature_importances_\n",
    "feat_labels = X_train.columns[0:]\n",
    "indices = np . argsort(importances)[::-1 ] \n",
    "for f in range(X_train .shape[1]):\n",
    "    if importances[indices[f]] < 0.000011:\n",
    "        col_etc.append(feat_labels[indices[f]])\n",
    "    print (\"%2d) %- *s %f \" % (f + 1, 30 , feat_labels[indices[f]] , importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=20, max_features=7, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=3, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_best = X_train.drop(col_etc,axis=1)\n",
    "best_etc.fit(X_best, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_best = X_test.drop(col_etc,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hetc2 = best_etc.predict_proba(X_test_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7516915208622221"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_hetc2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_best = Xt.drop(col_etc,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_etc2 = best_etc.predict_proba(Xt_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_bg = pd.DataFrame({'_ID_': yt.index})\n",
    "submission_bg['_VAL_'] = y_pred_etc2[:,1]\n",
    "submission_bg.to_csv('submission_etc12345.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
